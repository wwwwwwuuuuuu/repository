模型选择、过拟合和欠拟合
模型选择综合考虑训练误差和泛化误差，其中泛化误差用测试集上的误差来近似。
实际应用中通常设置验证数据集，用于超参数的选择。但在样本集数量较小的时候，验证数据集结果具有偶然性，因此采用K折交叉验证的方法。
过拟合：模型的训练误差远远小于它在测试数据集上的误差，可以通过权重衰减、丢弃法来应对。训练数据集中的样本数目过少，特别是比模型参数数量（按元素计）更少时，容易发生过拟合
欠拟合：无法得到较低的训练误差，可以通过增加模型复杂度来应对。
泛化误差不会随着训练数据集里样本数量增大而增加

权重衰减
应对L2范数正则化，正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小

丢弃法
倒置丢弃法：一般只有训练模型使用丢弃法，测试模型时，为了拿到更加确定性的结果，一般不使用dropout。
对于多层感知机，一般将靠近输入层的丢弃概率设置的小一点

梯度消失与梯度爆炸
模型训练时可能遇见的问题
当神经网络的层数较多时，模型的稳定性容易变差，可能发生梯度消失和梯度爆炸(裁剪梯度可以应对)。
随机初始化模型参数：因神经网络模型在层间个个单元之间具有对称性。如果每个隐藏单元的参数都初始化为相等的值，本质上只有一个隐藏单元在发挥作用，所以随机初始化。

模型应用时可能遇见的问题（因正确的训练与测试应该服从同分布）
协变量偏移：输入的分布P(X)可能随时间而改变，但标记函数，即条件分布P(Y|X)不会改变。e.g 训练时候使用猫和狗的真是图片，测试的时候使用卡通图像
标签偏移：标签P(Y)上的边缘分布发生变化，但类条件分布P(X|Y)是不变的。e.g 病因（要预测的结果）导致症状（观察到的结果），训练数据集包含流感p(y)的样本，而测试数据集中有流感p(y)和流感q(y)，其中不变的是P(X|Y)流感症状——可理解为测试集出现了训练时没有的标签
概念偏移：标签的定义本身发生改变，通常只为逐渐变化

循环神经网络进阶
门控循环单元（GRU）
当时间步数较大或者时间步较小时，循环神经网络的梯度容易出现衰减或爆炸。循环神经网络在实际中难以捕捉时间序列中时间步距离较大的依赖关系
重置门 Rt
更新门 Zt
候选隐藏状态：H~t=tanh(XW+(Rt dot. H(t-1))W+B)
Ht=Zt dot. H(t-1)+(1-Zt) dot. H~t
总结：重置门有助于捕捉时间序列中短期的依赖关系，更新门有助于捕捉时间序列里长期的依赖关系
长短期记忆（LSTM）
遗忘门：控制上一时间步的记忆细胞
输入门：控制当前时间步的输入
输出门：控制从记忆细胞到隐藏状态
记忆细胞：一种特殊的隐藏状态的信息的流动
Formula:

 
记忆细胞Ct相当于长期记忆，Ht相当于短期记忆。
参考讲解笔记：

深度循环神经网络（含有多个隐藏层的循环神经网络）
双向循环神经网络：每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入）
