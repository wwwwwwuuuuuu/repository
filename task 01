线性回归
单层神经网络，且为全连接层
适用于output连续的场景
解析解&数值解：采用数值解迭代优化
小批量随机梯度下降法：随机均匀采样一个由固定数目训练数据样本所组成的小批量B，然后求小批量中数据样本的平均损失的有关模型参数的导数
Pytorch中l.backtrack()函数前，l需要求和l.sum()。（与mxnet不一样）


Softmax回归
单层神经网络，且为全连接层
适用于output离散的场景
Softmax：将输出值变为值为正且和为1的概率分布   yi=exp(oi)/∑exp(oj)
交叉熵损失函数（用于衡量两个概率分布的差异）：最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率


多层感知机
激活函数（为了防止其与单层神经网络等价，添加非线性函数）：常用的激活函数包括ReLU函数，sigmoid函数，tanh函数
多层感知机是含有至少一个隐藏层的由全连接层组成的神经网络
激活函数的选择经验：
ReLU在大多数情况下使用，因为计算简单，但是只在隐藏层使用。
用于分类器问题时，sigmoid函数及其组合函数效果比较好，但由于梯度消失问题有时候避免使用sigmoid和tanh
